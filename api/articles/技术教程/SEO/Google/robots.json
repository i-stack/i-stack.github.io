{"title":"robots","slug":"技术教程/SEO/Google/robots","date":"2021-01-08T12:46:24.000Z","updated":"2023-08-19T16:33:46.648Z","comments":true,"path":"api/articles/技术教程/SEO/Google/robots.json","excerpt":null,"covers":null,"content":"<link rel=\"stylesheet\" class=\"aplayer-secondary-style-marker\" href=\"/assets/css/APlayer.min.css\"><script src=\"/assets/js/APlayer.min.js\" class=\"aplayer-secondary-script-marker\"></script><script class=\"meting-secondary-script-marker\" src=\"/assets/js/Meting.min.js\"></script><h2 id=\"robots-txt-简介\"><a href=\"#robots-txt-简介\" class=\"headerlink\" title=\"robots.txt 简介\"></a>robots.txt 简介</h2><p>robots.txt 文件规定了搜索引擎抓取工具可以访问您网站上的哪些网址。 此文件主要用于避免您的网站收到过多请求；它并不是一种阻止 Google 抓取某个网页的机制。若想阻止 Google 访问某个网页，请使用 noindex 禁止将其编入索引，或使用密码保护该网页。</p>\n<h2 id=\"robots-txt-文件有何用途？\"><a href=\"#robots-txt-文件有何用途？\" class=\"headerlink\" title=\"robots.txt 文件有何用途？\"></a>robots.txt 文件有何用途？</h2><p>robots.txt 文件主要用于管理流向您网站的抓取工具流量，通常用于阻止 Google 访问某个文件（具体取决于文件类型）。</p>\n<p><strong>robots.txt 对不同文件类型的影响</strong></p>\n<p><b>网页:</b></p>\n<ul>\n<li>对于网页（包括 HTML、PDF，或其他 Google 能够读取的非媒体格式），可在以下情况下使用 robots.txt 文件管理抓取流量：<ul>\n<li>Google 抓取工具的请求会导致服务器超负荷；</li>\n<li>不想让 Google 抓取您网站上的不重要网页或相似网页。</li>\n</ul>\n</li>\n<li>如果您使用 robots.txt 文件阻止 Google 抓取您的网页，则其网址仍可能会显示在搜索结果中，但搜索结果不会包含对该网页的说明。 而且，图片文件、视频文件、PDF 文件和其他非 HTML 文件都会被排除在外。如果您看到了这样一条与您网页对应的搜索结果并想修正它，请移除屏蔽该网页的 robots.txt 条目。</li>\n</ul>\n<p><b>媒体文件:</b></p>\n<p>使用 robots.txt 文件管理抓取流量并阻止图片、视频和音频文件出现在 Google 搜索结果中。这不会阻止其他网页或用户链接到您的图片/视频/音频文件。</p>\n<p><b>资源文件:</b></p>\n<p>如果在加载网页时跳过诸如不重要的图片、脚本或样式文件之类的资源不会对网页造成太大影响，可以使用 robots.txt 文件屏蔽此类资源。不过，如果缺少此类资源会导致 Google 抓取工具更难解读网页，请勿屏蔽此类资源，否则 Google 将无法有效分析有赖于此类资源的网页。</p>\n<h2 id=\"了解-robots-txt-文件的限制\"><a href=\"#了解-robots-txt-文件的限制\" class=\"headerlink\" title=\"了解 robots.txt 文件的限制\"></a>了解 robots.txt 文件的限制</h2><p>在创建或修改 robots.txt 文件之前，您应了解这种网址屏蔽方法的限制。根据您的目标和具体情况，您可能需要考虑采用其他机制来确保搜索引擎无法在网络上找到您的网址。</p>\n<ul>\n<li><b>并非所有搜索引擎都支持 robots.txt 规则。</b><br>robots.txt 文件中的命令并不能强制规范抓取工具对网站采取的行为；是否遵循这些命令由抓取工具自行决定。Googlebot 和其他正规的网页抓取工具都会遵循 robots.txt 文件中的命令，但其他抓取工具未必如此。因此，如果您想确保特定信息不会被网页抓取工具抓取，我们建议您采用其他屏蔽方法，例如用密码保护服务器上的隐私文件。</li>\n<li><b>不同的抓取工具会以不同的方式解析语法。</b><br>虽然正规的网页抓取工具会遵循 robots.txt 文件中的规则，但每种抓取工具可能会以不同的方式解析这些规则。您需要好好了解一下适用于不同网页抓取工具的正确语法，因为有些抓取工具可能会无法理解某些命令。</li>\n<li><b>如果其他网站上有链接指向被 robots.txt 文件屏蔽的网页，则此网页仍可能会被编入索引。</b><br>尽管 Google 不会抓取被 robots.txt 文件屏蔽的内容或将其编入索引，但如果网络上的其他位置有链接指向被禁止访问的网址，我们仍可能会找到该网址并将其编入索引。因此，相关网址和其他公开显示的信息（如相关页面链接中的定位文字）仍可能会出现在 Google 搜索结果中。若要正确阻止您的网址出现在 Google 搜索结果中，您应为服务器上的文件设置密码保护、使用 noindex meta 标记或响应标头，或者彻底移除网页。</li>\n</ul>\n<h2 id=\"创建或更新-robots-txt-文件\"><a href=\"#创建或更新-robots-txt-文件\" class=\"headerlink\" title=\"创建或更新 robots.txt 文件\"></a>创建或更新 robots.txt 文件</h2><p><b>以下是该 robots.txt 文件的含义：</b></p>\n<ol>\n<li><p>名为 Googlebot 的用户代理不能抓取任何以 <code>https://example.com/nogooglebot/</code> 开头的网址。</p>\n</li>\n<li><p>其他所有用户代理均可抓取整个网站。不指定这条规则也无妨，结果是一样的；默认行为是用户代理可以抓取整个网站。</p>\n</li>\n<li><p>该网站的站点地图文件路径为 <code>https://www.example.com/sitemap.xml</code>。</p>\n</li>\n</ol>\n<p><strong>要创建 robots.txt 文件并使其在一般情况下具备可访问性和实用性，需要完成 4 个步骤：</strong></p>\n<ol>\n<li><a href=\"#%E5%88%9B%E5%BB%BA-robotstxt-%E6%96%87%E4%BB%B6\">创建一个名为 robots.txt 的文件</a></li>\n<li><a href=\"#%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99-robotstxt-%E8%A7%84%E5%88%99\">向 robots.txt 文件添加规则</a></li>\n<li>[将 robots.txt 文件上传到网站的根目录]。</li>\n<li>测试 robots.txt 文件。</li>\n</ol>\n<h3 id=\"创建-robots-txt-文件\"><a href=\"#创建-robots-txt-文件\" class=\"headerlink\" title=\"创建 robots.txt 文件\"></a>创建 robots.txt 文件</h3><p>可以使用任意文本编辑器创建 robots.txt 文件。例如，Notepad、TextEdit、vi 和 emacs 可用来创建有效的 robots.txt 文件。如果保存文件时出现相应系统提示，请务必使用 UTF-8 编码保存文件。</p>\n<p><strong>格式和位置规则：</strong></p>\n<ul>\n<li>文件必须命名为 robots.txt。</li>\n<li>网站只能有 1 个 robots.txt 文件。</li>\n<li>robots.txt 文件必须位于其要应用到的网站主机的根目录下。例如，若要控制对 <code>https://www.example.com</code> 下所有网址的抓取，就必须将 robots.txt 文件放在 <code>https://www.example.com/robots.txt</code> 下，一定不能将其放在子目录中（例如 <code>https://example.com/pages/robots.txt</code> 下）。如果您不确定如何访问自己的网站根目录，或者需要相应权限才能访问，请与网站托管服务提供商联系。如果您无法访问网站根目录，请改用其他屏蔽方法（例如 meta 标记）。</li>\n<li>robots.txt 文件可以位于子网域（例如 <code>https://website.example.com/robots.txt</code>）或非标准端口（例如 <code>https://example.com:8181/robots.txt</code>）上。</li>\n<li>robots.txt 文件仅适用于所在的协议、主机和端口内的路径。也就是说，<code>https://example.com/robots.txt</code> 中的规则仅适用于 <code>https://example.com/</code> 中的文件，而不适用于子网域（如 <code>https://m.example.com/</code>）或备用协议（如 <code>http://example.com/</code>）。</li>\n<li>robots.txt 文件必须是采用 UTF-8 编码（包括 ASCII）的文本文件。Google 可能会忽略不属于 UTF-8 范围的字符，从而可能会导致 robots.txt 规则无效。</li>\n</ul>\n<h3 id=\"如何编写-robots-txt-规则\"><a href=\"#如何编写-robots-txt-规则\" class=\"headerlink\" title=\"如何编写 robots.txt 规则\"></a>如何编写 robots.txt 规则</h3><p>规则是关于抓取工具可以抓取网站哪些部分的说明。向 robots.txt 文件中添加规则时，请遵循以下准则：</p>\n<ul>\n<li>robots.txt 文件由一个或多个组（一组规则）组成。</li>\n<li>每个组由多条规则（也称为指令）组成，每条规则各占一行。每个组都以 User-agent 行开头，该行指定了组适用的目标。每个组包含以下信息：<ul>\n<li>组的适用对象（用户代理）</li>\n<li>代理可以访问的目录或文件。</li>\n<li>代理无法访问的目录或文件。</li>\n</ul>\n</li>\n<li>抓取工具会按从上到下的顺序处理组。一个用户代理只能匹配 1 个规则集（即与相应用户代理匹配的首个最具体组）。如果同一用户代理有多个组，这些组会在处理之前合并到一个组中。</li>\n<li>系统的默认假设是：用户代理可以抓取所有未被 disallow 规则屏蔽的网页或目录。</li>\n<li>规则区分大小写。例如，disallow: /file.asp 适用于 <code>https://www.example.com/file.asp</code>，但不适用于 <code>https://www.example.com/FILE.asp</code>。</li>\n<li># 字符表示注释的开始处。在处理过程中，系统会忽略注释。</li>\n</ul>\n<p><b>Google 的抓取工具支持 robots.txt 文件中的以下规则：</b></p>\n<ul>\n<li>user-agent: [必需，每个组需含一个或多个 User-agent 条目] 该规则指定了规则适用的自动客户端（即搜索引擎抓取工具）的名称。这是每个规则组的首行内容。Google 用户代理列表中列出了 Google 用户代理名称。使用星号 (*) 会匹配除各种 AdsBot 抓取工具之外的所有抓取工具，AdsBot 抓取工具必须明确指定。例如：<figure class=\"highlight plaintext\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Example 1: Block only Googlebot</span><br><span class=\"line\">User-agent: Googlebot</span><br><span class=\"line\">Disallow: /</span><br><span class=\"line\"></span><br><span class=\"line\"># Example 2: Block Googlebot and Adsbot</span><br><span class=\"line\">User-agent: Googlebot</span><br><span class=\"line\">User-agent: AdsBot-Google</span><br><span class=\"line\">Disallow: /</span><br><span class=\"line\"></span><br><span class=\"line\"># Example 3: Block all crawlers except AdsBot (AdsBot crawlers must be named explicitly)</span><br><span class=\"line\">User-agent: *</span><br><span class=\"line\">Disallow: /</span><br></pre></td></tr></tbody></table></figure></li>\n<li>disallow: [每条规则需含至少一个或多个 disallow 或 allow 条目] 您不希望用户代理抓取的目录或网页（相对于根网域而言）。如果规则引用了某个网页，则必须提供浏览器中显示的完整网页名称。它必须以 / 字符开头；如果它引用了某个目录，则必须以 / 标记结尾。</li>\n<li>allow: [每条规则需含至少一个或多个 disallow 或 allow 条目] 上文中提到的用户代理可以抓取的目录或网页（相对于根网域而言）。此规则用于替换 disallow 规则，从而允许抓取已禁止访问的目录中的子目录或网页。对于单个网页，请指定浏览器中显示的完整网页名称。它必须以 / 字符开头；如果它引用了某个目录，则必须以 / 标记结尾。</li>\n<li>sitemap: [可选，每个文件可含零个或多个 sitemap 条目] 相应网站的站点地图的位置。站点地图网址必须是完全限定的网址；Google 不会假定存在或检查是否存在 http、https、www、非 www 网址变体。站点地图是一种用于指示 Google 应抓取哪些内容的理想方式，但并不用于指示 Google 可以抓取或不能抓取哪些内容。示例：<figure class=\"highlight plaintext\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sitemap: https://example.com/sitemap.xml</span><br><span class=\"line\">Sitemap: https://www.example.com/sitemap.xml</span><br></pre></td></tr></tbody></table></figure>\n除 sitemap 之外的所有规则都支持使用通配符 * 表示路径前缀、后缀或整个字符串。</li>\n</ul>\n<table class=\"responsive\">\n    <tbody>\n        <tr>\n            <th colspan=\"2\">实用规则</th>\n        </tr>\n        <tr>\n            <td>禁止抓取整个网站</td>\n            <td>\n                <p>请注意，在某些情况下，Google 即使未抓取网站的网址，仍可能将其编入索引。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                    <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>禁止抓取某一目录及其内容</td>\n            <td>\n                <p>在目录名后添加一道正斜线，即可禁止抓取整个目录。</p>\n                <aside class=\"caution\"><b>注意</b>：请勿使用 robots.txt 禁止访问私密内容；请改用正确的身份验证机制。对于 robots.txt 文件所禁止抓取的网址，Google 仍可能会在不进行抓取的情况下将其编入索引；另外，由于 robots.txt 文件可供任何人随意查看，因此可能会泄露您的私密内容的位置。\n                </aside>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /calendar/\nDisallow: /junk/\nDisallow: /books/fiction/contemporary/</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>仅允许某一抓取工具访问网站内容</td>\n            <td>\n                <p>只有 <code dir=\"ltr\" translate=\"no\">googlebot-news</code> 可以抓取整个网站。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot-news\nAllow: /\nUser-agent: *\nDisallow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>允许除某一抓取工具以外的其他所有抓取工具访问网站内容</td>\n            <td>\n                <p><code dir=\"ltr\" translate=\"no\">Unnecessarybot</code> 不能抓取相应网站，所有其他漫游器都可以。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Unnecessarybot\nDisallow: /\nUser-agent: *\nAllow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td><p>禁止抓取某一网页</p></td>\n            <td>\n                <p>例如，禁止抓取位于 <code dir=\"ltr\" translate=\"no\">https://example.com/useless_file.html</code> 的 <code dir=\"ltr\" translate=\"no\">useless_file.html</code> 页面和 <code dir=\"ltr\" translate=\"no\">junk</code> 目录中的 <code dir=\"ltr\" translate=\"no\">other_useless_file.html</code>。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /useless_file.html\nDisallow: /junk/other_useless_file.html</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止抓取除子目录以外的整个网站</p>\n            </td>\n            <td>\n                <p>抓取工具只能访问 <code dir=\"ltr\" translate=\"no\">public</code> 子目录。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /\nAllow: /public/</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止 Google 图片访问某一特定图片</p>\n            </td>\n            <td>\n                <p>例如，禁止访问 <code dir=\"ltr\" translate=\"no\">dogs.jpg</code> 图片。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot-Image\nDisallow: /images/dogs.jpg</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止 Google 图片访问您网站上的所有图片</p>\n            </td>\n            <td>\n                <p>如果无法抓取图片和视频，则 Google 无法将其编入索引。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot-Image\nDisallow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止抓取某一特定文件类型的文件</p>\n            </td>\n            <td>\n                <p>例如，禁止抓取所有 <code dir=\"ltr\" translate=\"no\">.gif</code> 文件。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot\nDisallow: /*.gif$</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止抓取整个网站，但允许 <code dir=\"ltr\" translate=\"no\">Mediapartners-Google</code> 访问内容</p>\n            </td>\n            <td>\n                <p>\n                    实施此规则会阻止您的网页显示在搜索结果中，但 <code dir=\"ltr\" translate=\"no\">Mediapartners-Google</code> 网页抓取工具仍能分析这些网页，以确定要向访问您网站的用户显示哪些广告。\n                </p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /\nUser-agent: Mediapartners-Google\nAllow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>使用 <code dir=\"ltr\" translate=\"no\">*</code> 和 <code dir=\"ltr\" translate=\"no\">$</code> 通配符匹配以特定字符串结尾的网址</td>\n            <td>\n                <p>例如，禁止抓取所有 <code dir=\"ltr\" translate=\"no\">.xls</code> 文件。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot\nDisallow: /*.xls$</pre>\n                </devsite-code>\n            </td>\n        </tr>\n    </tbody>\n</table>\n\n<h3 id=\"上传-robots-txt-文件\"><a href=\"#上传-robots-txt-文件\" class=\"headerlink\" title=\"上传 robots.txt 文件\"></a>上传 robots.txt 文件</h3><p><strong>robots.txt 文件应位于网站的根目录下</strong>。因此，对于网站 <a href=\"http://www.example.com,robots.txt/\">www.example.com，robots.txt</a> 文件的路径应为 <a href=\"http://www.example.com/robots.txt%E3%80%82robots.txt\">www.example.com/robots.txt。robots.txt</a> 是一种遵循漫游器排除标准的纯文本文件，由一条或多条规则组成。每条规则可禁止或允许所有或特定抓取工具抓取托管 robots.txt 文件的网域或子网域上的指定文件路径。</p>\n<h3 id=\"测试-robots-txt-标记\"><a href=\"#测试-robots-txt-标记\" class=\"headerlink\" title=\"测试 robots.txt 标记\"></a>测试 robots.txt 标记</h3><p>要测试新上传的 robots.txt 文件是否可公开访问，请在浏览器中打开无痕浏览窗口（或等效窗口），然后转到 robots.txt 文件的位置。例如 <code>https://example.com/robots.txt</code>。如果您看到 robots.txt 文件的内容，就可准备测试标记了。</p>\n<h3 id=\"向-Google-提交-robots-txt-文件\"><a href=\"#向-Google-提交-robots-txt-文件\" class=\"headerlink\" title=\"向 Google 提交 robots.txt 文件\"></a>向 Google 提交 robots.txt 文件</h3><p>在上传并测试 robots.txt 文件后，Google 的抓取工具会自动找到并开始使用您的 robots.txt 文件。</p>\n<h3 id=\"刷新-Google-的-robots-txt-缓存\"><a href=\"#刷新-Google-的-robots-txt-缓存\" class=\"headerlink\" title=\"刷新 Google 的 robots.txt 缓存\"></a>刷新 Google 的 robots.txt 缓存</h3><p>在自动抓取过程中，Google 的抓取工具会发现您对 robots.txt 文件所做的更改，并每 24 小时更新一次缓存的版本。如果您需要更快地更新缓存，请使用 robots.txt <a href=\"https://support.google.com/webmasters/answer/6062598?hl=zh-Hans\">测试工具</a>的提交功能。</p>\n<ol>\n<li>点击查看更新后的版本，查看您当前使用的 robots.txt 是否是您希望 Google 抓取的版本。</li>\n<li>点击提交，通知 Google 您已更改 robots.txt 文件，并请求 Google 抓取该文件。</li>\n<li>在浏览器中刷新页面，以便更新该工具的编辑器并查看您当前使用的 robots.txt 代码，从而确定 Google 是否已成功抓取最新版本的 robots.txt 文件。在刷新页面后，您还可以点击下拉菜单，以便查看 Google 首次发现您的 robots.txt 文件最新版本时的时间戳。</li>\n</ol>\n","more":"<h2 id=\"robots-txt-简介\"><a href=\"#robots-txt-简介\" class=\"headerlink\" title=\"robots.txt 简介\"></a>robots.txt 简介</h2><p>robots.txt 文件规定了搜索引擎抓取工具可以访问您网站上的哪些网址。 此文件主要用于避免您的网站收到过多请求；它并不是一种阻止 Google 抓取某个网页的机制。若想阻止 Google 访问某个网页，请使用 noindex 禁止将其编入索引，或使用密码保护该网页。</p>\n<h2 id=\"robots-txt-文件有何用途？\"><a href=\"#robots-txt-文件有何用途？\" class=\"headerlink\" title=\"robots.txt 文件有何用途？\"></a>robots.txt 文件有何用途？</h2><p>robots.txt 文件主要用于管理流向您网站的抓取工具流量，通常用于阻止 Google 访问某个文件（具体取决于文件类型）。</p>\n<p><strong>robots.txt 对不同文件类型的影响</strong></p>\n<p><b>网页:</b></p>\n<ul>\n<li>对于网页（包括 HTML、PDF，或其他 Google 能够读取的非媒体格式），可在以下情况下使用 robots.txt 文件管理抓取流量：<ul>\n<li>Google 抓取工具的请求会导致服务器超负荷；</li>\n<li>不想让 Google 抓取您网站上的不重要网页或相似网页。</li>\n</ul>\n</li>\n<li>如果您使用 robots.txt 文件阻止 Google 抓取您的网页，则其网址仍可能会显示在搜索结果中，但搜索结果不会包含对该网页的说明。 而且，图片文件、视频文件、PDF 文件和其他非 HTML 文件都会被排除在外。如果您看到了这样一条与您网页对应的搜索结果并想修正它，请移除屏蔽该网页的 robots.txt 条目。</li>\n</ul>\n<p><b>媒体文件:</b></p>\n<p>使用 robots.txt 文件管理抓取流量并阻止图片、视频和音频文件出现在 Google 搜索结果中。这不会阻止其他网页或用户链接到您的图片&#x2F;视频&#x2F;音频文件。</p>\n<p><b>资源文件:</b></p>\n<p>如果在加载网页时跳过诸如不重要的图片、脚本或样式文件之类的资源不会对网页造成太大影响，可以使用 robots.txt 文件屏蔽此类资源。不过，如果缺少此类资源会导致 Google 抓取工具更难解读网页，请勿屏蔽此类资源，否则 Google 将无法有效分析有赖于此类资源的网页。</p>\n<h2 id=\"了解-robots-txt-文件的限制\"><a href=\"#了解-robots-txt-文件的限制\" class=\"headerlink\" title=\"了解 robots.txt 文件的限制\"></a>了解 robots.txt 文件的限制</h2><p>在创建或修改 robots.txt 文件之前，您应了解这种网址屏蔽方法的限制。根据您的目标和具体情况，您可能需要考虑采用其他机制来确保搜索引擎无法在网络上找到您的网址。</p>\n<ul>\n<li><b>并非所有搜索引擎都支持 robots.txt 规则。</b><br>robots.txt 文件中的命令并不能强制规范抓取工具对网站采取的行为；是否遵循这些命令由抓取工具自行决定。Googlebot 和其他正规的网页抓取工具都会遵循 robots.txt 文件中的命令，但其他抓取工具未必如此。因此，如果您想确保特定信息不会被网页抓取工具抓取，我们建议您采用其他屏蔽方法，例如用密码保护服务器上的隐私文件。</li>\n<li><b>不同的抓取工具会以不同的方式解析语法。</b><br>虽然正规的网页抓取工具会遵循 robots.txt 文件中的规则，但每种抓取工具可能会以不同的方式解析这些规则。您需要好好了解一下适用于不同网页抓取工具的正确语法，因为有些抓取工具可能会无法理解某些命令。</li>\n<li><b>如果其他网站上有链接指向被 robots.txt 文件屏蔽的网页，则此网页仍可能会被编入索引。</b><br>尽管 Google 不会抓取被 robots.txt 文件屏蔽的内容或将其编入索引，但如果网络上的其他位置有链接指向被禁止访问的网址，我们仍可能会找到该网址并将其编入索引。因此，相关网址和其他公开显示的信息（如相关页面链接中的定位文字）仍可能会出现在 Google 搜索结果中。若要正确阻止您的网址出现在 Google 搜索结果中，您应为服务器上的文件设置密码保护、使用 noindex meta 标记或响应标头，或者彻底移除网页。</li>\n</ul>\n<h2 id=\"创建或更新-robots-txt-文件\"><a href=\"#创建或更新-robots-txt-文件\" class=\"headerlink\" title=\"创建或更新 robots.txt 文件\"></a>创建或更新 robots.txt 文件</h2><p><b>以下是该 robots.txt 文件的含义：</b></p>\n<ol>\n<li><p>名为 Googlebot 的用户代理不能抓取任何以 <code>https://example.com/nogooglebot/</code> 开头的网址。</p>\n</li>\n<li><p>其他所有用户代理均可抓取整个网站。不指定这条规则也无妨，结果是一样的；默认行为是用户代理可以抓取整个网站。</p>\n</li>\n<li><p>该网站的站点地图文件路径为 <code>https://www.example.com/sitemap.xml</code>。</p>\n</li>\n</ol>\n<p><strong>要创建 robots.txt 文件并使其在一般情况下具备可访问性和实用性，需要完成 4 个步骤：</strong></p>\n<ol>\n<li><a href=\"#%E5%88%9B%E5%BB%BA-robotstxt-%E6%96%87%E4%BB%B6\">创建一个名为 robots.txt 的文件</a></li>\n<li><a href=\"#%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99-robotstxt-%E8%A7%84%E5%88%99\">向 robots.txt 文件添加规则</a></li>\n<li>[将 robots.txt 文件上传到网站的根目录]。</li>\n<li>测试 robots.txt 文件。</li>\n</ol>\n<h3 id=\"创建-robots-txt-文件\"><a href=\"#创建-robots-txt-文件\" class=\"headerlink\" title=\"创建 robots.txt 文件\"></a>创建 robots.txt 文件</h3><p>可以使用任意文本编辑器创建 robots.txt 文件。例如，Notepad、TextEdit、vi 和 emacs 可用来创建有效的 robots.txt 文件。如果保存文件时出现相应系统提示，请务必使用 UTF-8 编码保存文件。</p>\n<p><strong>格式和位置规则：</strong></p>\n<ul>\n<li>文件必须命名为 robots.txt。</li>\n<li>网站只能有 1 个 robots.txt 文件。</li>\n<li>robots.txt 文件必须位于其要应用到的网站主机的根目录下。例如，若要控制对 <code>https://www.example.com</code> 下所有网址的抓取，就必须将 robots.txt 文件放在 <code>https://www.example.com/robots.txt</code> 下，一定不能将其放在子目录中（例如 <code>https://example.com/pages/robots.txt</code> 下）。如果您不确定如何访问自己的网站根目录，或者需要相应权限才能访问，请与网站托管服务提供商联系。如果您无法访问网站根目录，请改用其他屏蔽方法（例如 meta 标记）。</li>\n<li>robots.txt 文件可以位于子网域（例如 <code>https://website.example.com/robots.txt</code>）或非标准端口（例如 <code>https://example.com:8181/robots.txt</code>）上。</li>\n<li>robots.txt 文件仅适用于所在的协议、主机和端口内的路径。也就是说，<code>https://example.com/robots.txt</code> 中的规则仅适用于 <code>https://example.com/</code> 中的文件，而不适用于子网域（如 <code>https://m.example.com/</code>）或备用协议（如 <code>http://example.com/</code>）。</li>\n<li>robots.txt 文件必须是采用 UTF-8 编码（包括 ASCII）的文本文件。Google 可能会忽略不属于 UTF-8 范围的字符，从而可能会导致 robots.txt 规则无效。</li>\n</ul>\n<h3 id=\"如何编写-robots-txt-规则\"><a href=\"#如何编写-robots-txt-规则\" class=\"headerlink\" title=\"如何编写 robots.txt 规则\"></a>如何编写 robots.txt 规则</h3><p>规则是关于抓取工具可以抓取网站哪些部分的说明。向 robots.txt 文件中添加规则时，请遵循以下准则：</p>\n<ul>\n<li>robots.txt 文件由一个或多个组（一组规则）组成。</li>\n<li>每个组由多条规则（也称为指令）组成，每条规则各占一行。每个组都以 User-agent 行开头，该行指定了组适用的目标。每个组包含以下信息：<ul>\n<li>组的适用对象（用户代理）</li>\n<li>代理可以访问的目录或文件。</li>\n<li>代理无法访问的目录或文件。</li>\n</ul>\n</li>\n<li>抓取工具会按从上到下的顺序处理组。一个用户代理只能匹配 1 个规则集（即与相应用户代理匹配的首个最具体组）。如果同一用户代理有多个组，这些组会在处理之前合并到一个组中。</li>\n<li>系统的默认假设是：用户代理可以抓取所有未被 disallow 规则屏蔽的网页或目录。</li>\n<li>规则区分大小写。例如，disallow: &#x2F;file.asp 适用于 <code>https://www.example.com/file.asp</code>，但不适用于 <code>https://www.example.com/FILE.asp</code>。</li>\n<li># 字符表示注释的开始处。在处理过程中，系统会忽略注释。</li>\n</ul>\n<p><b>Google 的抓取工具支持 robots.txt 文件中的以下规则：</b></p>\n<ul>\n<li>user-agent: [必需，每个组需含一个或多个 User-agent 条目] 该规则指定了规则适用的自动客户端（即搜索引擎抓取工具）的名称。这是每个规则组的首行内容。Google 用户代理列表中列出了 Google 用户代理名称。使用星号 (*) 会匹配除各种 AdsBot 抓取工具之外的所有抓取工具，AdsBot 抓取工具必须明确指定。例如：<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Example 1: Block only Googlebot</span><br><span class=\"line\">User-agent: Googlebot</span><br><span class=\"line\">Disallow: /</span><br><span class=\"line\"></span><br><span class=\"line\"># Example 2: Block Googlebot and Adsbot</span><br><span class=\"line\">User-agent: Googlebot</span><br><span class=\"line\">User-agent: AdsBot-Google</span><br><span class=\"line\">Disallow: /</span><br><span class=\"line\"></span><br><span class=\"line\"># Example 3: Block all crawlers except AdsBot (AdsBot crawlers must be named explicitly)</span><br><span class=\"line\">User-agent: *</span><br><span class=\"line\">Disallow: /</span><br></pre></td></tr></table></figure></li>\n<li>disallow: [每条规则需含至少一个或多个 disallow 或 allow 条目] 您不希望用户代理抓取的目录或网页（相对于根网域而言）。如果规则引用了某个网页，则必须提供浏览器中显示的完整网页名称。它必须以 &#x2F; 字符开头；如果它引用了某个目录，则必须以 &#x2F; 标记结尾。</li>\n<li>allow: [每条规则需含至少一个或多个 disallow 或 allow 条目] 上文中提到的用户代理可以抓取的目录或网页（相对于根网域而言）。此规则用于替换 disallow 规则，从而允许抓取已禁止访问的目录中的子目录或网页。对于单个网页，请指定浏览器中显示的完整网页名称。它必须以 &#x2F; 字符开头；如果它引用了某个目录，则必须以 &#x2F; 标记结尾。</li>\n<li>sitemap: [可选，每个文件可含零个或多个 sitemap 条目] 相应网站的站点地图的位置。站点地图网址必须是完全限定的网址；Google 不会假定存在或检查是否存在 http、https、www、非 www 网址变体。站点地图是一种用于指示 Google 应抓取哪些内容的理想方式，但并不用于指示 Google 可以抓取或不能抓取哪些内容。示例：<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Sitemap: https://example.com/sitemap.xml</span><br><span class=\"line\">Sitemap: https://www.example.com/sitemap.xml</span><br></pre></td></tr></table></figure>\n除 sitemap 之外的所有规则都支持使用通配符 * 表示路径前缀、后缀或整个字符串。</li>\n</ul>\n<table class=\"responsive\">\n    <tbody>\n        <tr>\n            <th colspan=\"2\">实用规则</th>\n        </tr>\n        <tr>\n            <td>禁止抓取整个网站</td>\n            <td>\n                <p>请注意，在某些情况下，Google 即使未抓取网站的网址，仍可能将其编入索引。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                    <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>禁止抓取某一目录及其内容</td>\n            <td>\n                <p>在目录名后添加一道正斜线，即可禁止抓取整个目录。</p>\n                <aside class=\"caution\"><b>注意</b>：请勿使用 robots.txt 禁止访问私密内容；请改用正确的身份验证机制。对于 robots.txt 文件所禁止抓取的网址，Google 仍可能会在不进行抓取的情况下将其编入索引；另外，由于 robots.txt 文件可供任何人随意查看，因此可能会泄露您的私密内容的位置。\n                </aside>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /calendar/\nDisallow: /junk/\nDisallow: /books/fiction/contemporary/</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>仅允许某一抓取工具访问网站内容</td>\n            <td>\n                <p>只有 <code dir=\"ltr\" translate=\"no\">googlebot-news</code> 可以抓取整个网站。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">\nUser-agent: Googlebot-news\nAllow: /\nUser-agent: *\nDisallow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>允许除某一抓取工具以外的其他所有抓取工具访问网站内容</td>\n            <td>\n                <p><code dir=\"ltr\" translate=\"no\">Unnecessarybot</code> 不能抓取相应网站，所有其他漫游器都可以。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">\nUser-agent: Unnecessarybot\nDisallow: /\nUser-agent: *\nAllow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td><p>禁止抓取某一网页</p></td>\n            <td>\n                <p>例如，禁止抓取位于 <code dir=\"ltr\" translate=\"no\">https://example.com/useless_file.html</code> 的 <code dir=\"ltr\" translate=\"no\">useless_file.html</code> 页面和 <code dir=\"ltr\" translate=\"no\">junk</code> 目录中的 <code dir=\"ltr\" translate=\"no\">other_useless_file.html</code>。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">\nUser-agent: *\nDisallow: /useless_file.html\nDisallow: /junk/other_useless_file.html</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止抓取除子目录以外的整个网站</p>\n            </td>\n            <td>\n                <p>抓取工具只能访问 <code dir=\"ltr\" translate=\"no\">public</code> 子目录。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\">\n                     <pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">\nUser-agent: *\nDisallow: /\nAllow: /public/</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止 Google 图片访问某一特定图片</p>\n            </td>\n            <td>\n                <p>例如，禁止访问 <code dir=\"ltr\" translate=\"no\">dogs.jpg</code> 图片。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot-Image\nDisallow: /images/dogs.jpg</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止 Google 图片访问您网站上的所有图片</p>\n            </td>\n            <td>\n                <p>如果无法抓取图片和视频，则 Google 无法将其编入索引。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot-Image\nDisallow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止抓取某一特定文件类型的文件</p>\n            </td>\n            <td>\n                <p>例如，禁止抓取所有 <code dir=\"ltr\" translate=\"no\">.gif</code> 文件。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot\nDisallow: /*.gif$</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>\n                <p>禁止抓取整个网站，但允许 <code dir=\"ltr\" translate=\"no\">Mediapartners-Google</code> 访问内容</p>\n            </td>\n            <td>\n                <p>\n                    实施此规则会阻止您的网页显示在搜索结果中，但 <code dir=\"ltr\" translate=\"no\">Mediapartners-Google</code> 网页抓取工具仍能分析这些网页，以确定要向访问您网站的用户显示哪些广告。\n                </p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: *\nDisallow: /\nUser-agent: Mediapartners-Google\nAllow: /</pre>\n                </devsite-code>\n            </td>\n        </tr>\n        <tr>\n            <td>使用 <code dir=\"ltr\" translate=\"no\">*</code> 和 <code dir=\"ltr\" translate=\"no\">$</code> 通配符匹配以特定字符串结尾的网址</td>\n            <td>\n                <p>例如，禁止抓取所有 <code dir=\"ltr\" translate=\"no\">.xls</code> 文件。</p>\n                <devsite-code no-copy=\"\" data-copy-event-label=\"\" dark-code=\"\"><pre translate=\"no\" dir=\"ltr\" is-upgraded=\"\">User-agent: Googlebot\nDisallow: /*.xls$</pre>\n                </devsite-code>\n            </td>\n        </tr>\n    </tbody>\n</table>\n\n<h3 id=\"上传-robots-txt-文件\"><a href=\"#上传-robots-txt-文件\" class=\"headerlink\" title=\"上传 robots.txt 文件\"></a>上传 robots.txt 文件</h3><p><strong>robots.txt 文件应位于网站的根目录下</strong>。因此，对于网站 <a href=\"http://www.example.com,robots.txt/\">www.example.com，robots.txt</a> 文件的路径应为 <a href=\"http://www.example.com/robots.txt%E3%80%82robots.txt\">www.example.com/robots.txt。robots.txt</a> 是一种遵循漫游器排除标准的纯文本文件，由一条或多条规则组成。每条规则可禁止或允许所有或特定抓取工具抓取托管 robots.txt 文件的网域或子网域上的指定文件路径。</p>\n<h3 id=\"测试-robots-txt-标记\"><a href=\"#测试-robots-txt-标记\" class=\"headerlink\" title=\"测试 robots.txt 标记\"></a>测试 robots.txt 标记</h3><p>要测试新上传的 robots.txt 文件是否可公开访问，请在浏览器中打开无痕浏览窗口（或等效窗口），然后转到 robots.txt 文件的位置。例如 <code>https://example.com/robots.txt</code>。如果您看到 robots.txt 文件的内容，就可准备测试标记了。</p>\n<h3 id=\"向-Google-提交-robots-txt-文件\"><a href=\"#向-Google-提交-robots-txt-文件\" class=\"headerlink\" title=\"向 Google 提交 robots.txt 文件\"></a>向 Google 提交 robots.txt 文件</h3><p>在上传并测试 robots.txt 文件后，Google 的抓取工具会自动找到并开始使用您的 robots.txt 文件。</p>\n<h3 id=\"刷新-Google-的-robots-txt-缓存\"><a href=\"#刷新-Google-的-robots-txt-缓存\" class=\"headerlink\" title=\"刷新 Google 的 robots.txt 缓存\"></a>刷新 Google 的 robots.txt 缓存</h3><p>在自动抓取过程中，Google 的抓取工具会发现您对 robots.txt 文件所做的更改，并每 24 小时更新一次缓存的版本。如果您需要更快地更新缓存，请使用 robots.txt <a href=\"https://support.google.com/webmasters/answer/6062598?hl=zh-Hans\">测试工具</a>的提交功能。</p>\n<ol>\n<li>点击查看更新后的版本，查看您当前使用的 robots.txt 是否是您希望 Google 抓取的版本。</li>\n<li>点击提交，通知 Google 您已更改 robots.txt 文件，并请求 Google 抓取该文件。</li>\n<li>在浏览器中刷新页面，以便更新该工具的编辑器并查看您当前使用的 robots.txt 代码，从而确定 Google 是否已成功抓取最新版本的 robots.txt 文件。在刷新页面后，您还可以点击下拉菜单，以便查看 Google 首次发现您的 robots.txt 文件最新版本时的时间戳。</li>\n</ol>\n","categories":[{"name":"技术教程","path":"api/categories/技术教程.json"}],"tags":[{"name":"SEO","path":"api/tags/SEO.json"}]}